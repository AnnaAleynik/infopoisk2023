{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2243,"status":"ok","timestamp":1679071243979,"user":{"displayName":"Анна Алейник","userId":"06222766668008807515"},"user_tz":-180},"id":"Lk4lxknXUbWl"},"outputs":[],"source":["import nltk\n","from nltk import sent_tokenize\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import pymystem3\n","from pymystem3 import Mystem\n","import re\n","from bs4 import BeautifulSoup\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19863,"status":"ok","timestamp":1679071408161,"user":{"displayName":"Анна Алейник","userId":"06222766668008807515"},"user_tz":-180},"id":"NUNkJYceUgAm","outputId":"a3f9ee4b-d735-429e-eccc-71c58a057547"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1679071411672,"user":{"displayName":"Анна Алейник","userId":"06222766668008807515"},"user_tz":-180},"id":"NDXFSqYbUh67"},"outputs":[],"source":["folder = \"/Учеба/infopoisk/task1\" \n","writeFolder = \"/Учеба/infopoisk/task2\" "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7793,"status":"ok","timestamp":1679071421433,"user":{"displayName":"Анна Алейник","userId":"06222766668008807515"},"user_tz":-180},"id":"9IzHFV4HUouZ","outputId":"13cec2ca-121b-4e25-f793-b2036c11598e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"]}],"source":["nltk.download(\"stopwords\")\n","nltk.download('punkt') # без этого не работает word_tokenize\n","stop_ru = stopwords.words('russian') # союзы и тд\n","stop_en = stopwords.words('english')\n","m = Mystem()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1679071424602,"user":{"displayName":"Анна Алейник","userId":"06222766668008807515"},"user_tz":-180},"id":"RNOo3KrIUryZ"},"outputs":[],"source":["# регулярка для вычищения слов с цифрами или символами внутри (моне*та, например)\n","# не ищет слова, оканчивающиеся символом или цифрой (чтобы оставить все слова с точками и запятыми)\n","CLEANR = re.compile('[a-zA-Zа-яА-Я]*[^a-zA-Zа-яА-Я\\s]+[a-zA-Zа-яА-Я]+')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96886,"status":"ok","timestamp":1679071527721,"user":{"displayName":"Анна Алейник","userId":"06222766668008807515"},"user_tz":-180},"id":"EmGvkYKMVD5P","outputId":"303b7647-ce44-4d0c-bab5-b1943bba33f9"},"outputs":[],"source":["tokens = []\n","lemmas = {}\n","\n","for dirpath, _, filenames in os.walk('/content/drive/My Drive/'+ folder):\n","  for filename in filenames:\n","    current_tokens = []\n","    current_lemmas = {}\n","    print(filename)\n","    with open('/content/drive/My Drive/'+ folder +'/' + filename, 'rb') as f:\n","        data = f.read()\n","        # очищает html тэги\n","        soup = BeautifulSoup(data, 'html.parser')\n","        for data in soup(['style', 'script']):\n","            # Remove tags\n","            data.decompose()\n","    \n","        # return data by retrieving the tag content\n","        answer = ' '.join(soup.stripped_strings)\n","      \n","        clean_answer = re.sub(CLEANR, ' ', answer)\n","        # убираем все цифры\n","        clean_answer = re.sub(r\"\\d+\", \" \", clean_answer)\n","        # удаляем знаки препинания\n","        clean_answer = re.sub(r'[^\\w\\s]', '', clean_answer)\n","        # выделяет токены - слова\n","        words = word_tokenize(clean_answer, language=\"russian\") \n","\n","        with open('/content/drive/My Drive/' + writeFolder + '/tokens-' + filename.replace(\".html\", \"\")  + '.txt', 'w') as f:\n","          with open('/content/drive/My Drive/' + writeFolder + '/lemmas-' + filename.replace(\".html\", \"\")  + '.txt', 'w') as lemma_file:\n","            for word in words:\n","                word = word.lower()\n","                if word not in stop_ru and word not in stop_en:\n","                    if word not in current_tokens:\n","                        current_tokens.append(word)\n","                        f.write(f\"{word}\\n\")   \n","\n","                    if word not in tokens:\n","                        tokens.append(word)\n","                    \n","                    lemma = m.lemmatize(word)[0]\n","                    if lemma not in current_lemmas.keys():\n","                      current_lemmas[lemma] = set()\n","                    current_lemmas[lemma].add(word)  \n","                    if lemma not in lemmas.keys():\n","                      lemmas[lemma] = set()\n","                    lemmas[lemma].add(word)\n","                    \n","            for lemma in current_lemmas.keys():\n","              lemma_file.write(f\"{lemma}: \")  \n","              for token in current_lemmas[lemma]:\n","                lemma_file.write(f\"{token} \")\n","              lemma_file.write(f\"\\n\")       "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1035,"status":"ok","timestamp":1679071548022,"user":{"displayName":"Анна Алейник","userId":"06222766668008807515"},"user_tz":-180},"id":"SMN4d5CkMR4z"},"outputs":[],"source":["with open('/content/drive/My Drive/' + writeFolder + '/tokens.txt', 'w') as f:\n","  for token in tokens:\n","        f.write(f\"{token}\\n\")       \n","\n","with open('/content/drive/My Drive/' + writeFolder + '/lemmas.txt', 'w') as f:\n","  for lemma in lemmas.keys():\n","        f.write(f\"{lemma}: \")  \n","        for token in lemmas[lemma]:\n","          f.write(f\"{token} \")\n","        f.write(f\"\\n\")       "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
