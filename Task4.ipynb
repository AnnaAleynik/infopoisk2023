{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEdmzJo-lzFt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import collections\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me8qEfBTfG2Z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbUb8chBfIrz"
      },
      "outputs": [],
      "source": [
        "inverted_index_folder = \"/4  курс/Информационный поиск/task3\"\n",
        "tokens_folder = \"/4  курс/Информационный поиск/task2\"\n",
        "write_folder = \"/4  курс/Информационный поиск/task4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7qtOpxvfL50"
      },
      "outputs": [],
      "source": [
        "token_inverted_index_count = {}\n",
        "tokens_dictionary = {}\n",
        "lemmas_dictionary = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Oc2CFUAfNqk"
      },
      "outputs": [],
      "source": [
        "files_count = len(tokens_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1A6uTPXfPHj"
      },
      "outputs": [],
      "source": [
        "# сохраняем все токены из всех файлов в словарь tokens_dictionary \n",
        "# {'0': ['token1', 'token2'], ...}\n",
        "for dirpath, _, filenames in os.walk('/content/drive/My Drive/'+ tokens_folder):\n",
        "  for filename in filenames:\n",
        "    if 'tokens-' in filename: \n",
        "      f = open('/content/drive/My Drive/'+ tokens_folder +'/' + filename, encoding='utf-8')\n",
        "      data = f.read()\n",
        "      name = filename.replace(\"tokens-\", \"\").replace(\".txt\", \"\")\n",
        "      data_list = data.splitlines()\n",
        "      tokens_dictionary[name] = data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oNn-gf7fQ8f"
      },
      "outputs": [],
      "source": [
        "# значения из 3 задания сохраняем в словарь token_inverted_index_count для удобной работы\n",
        "# {'token1': 10, ...}\n",
        "with open('/content/drive/My Drive/'+ inverted_index_folder + '/inverted_index_2.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    for elem in data:\n",
        "        elem = elem.replace(\"\\'\", \"\\\"\")\n",
        "        string_data = json.loads(elem)\n",
        "        token_inverted_index_count[string_data['word']] = string_data['count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt37EH1BfSdV"
      },
      "outputs": [],
      "source": [
        "for key, token_list in tokens_dictionary.items():\n",
        "    tf_idf_dictionary = {}\n",
        "\n",
        "    # считаем tf - отношение количества вхождений токена в файл ко всем токенам в файле\n",
        "    token_tf = collections.Counter(token_list)\n",
        "    token_list_len = len(token_list)\n",
        "    for i in token_tf:\n",
        "        token_tf[i] = token_tf[i]/float(token_list_len)\n",
        "\n",
        "    # считаем idf - отношение общего количества файлов к количеству файлов, содержащих токен\n",
        "    # считаем tf-idf - tf * idf\n",
        "    for word in token_tf:\n",
        "        idf = math.log10(files_count/token_inverted_index_count[word])\n",
        "\n",
        "        tf_idf_dictionary[word] = {\n",
        "            'idf': idf,\n",
        "            'tf-idf': token_tf[word] * idf\n",
        "        }\n",
        "\n",
        "    # запишем в файл\n",
        "    with open('/content/drive/My Drive/' + write_folder + '/tokens-' + key + '.txt', 'w') as f:\n",
        "        for token, metric in tf_idf_dictionary.items():\n",
        "            result_string = token + \" \" + str(metric['idf']) + \" \" + str(metric['tf-idf'])\n",
        "            f.write(\"%s\\n\" % result_string)\n",
        "        print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k1pS7S7fUrR"
      },
      "outputs": [],
      "source": [
        "# считаем для лемм\n",
        "\n",
        "for dirpath, _, filenames in os.walk('/content/drive/My Drive/'+ tokens_folder):\n",
        "  for filename in filenames:\n",
        "    if 'lemmas-' in filename: \n",
        "      f = open('/content/drive/My Drive/'+ tokens_folder +'/' + filename, encoding='utf-8')\n",
        "      data = f.read()\n",
        "      name = filename.replace(\"lemmas-\", \"\").replace(\".txt\", \"\")\n",
        "      data_list = data.splitlines()\n",
        "      lemmas_dictionary[name] = data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p49JIXE7fYvL"
      },
      "outputs": [],
      "source": [
        "for key, lemmas_list in lemmas_dictionary.items():\n",
        "  lemma_tf = {}\n",
        "  token_tf = collections.Counter(tokens_dictionary[key])\n",
        "  token_list_len = len(tokens_dictionary[key])\n",
        "\n",
        "  for v in lemmas_list:\n",
        "    v = v.strip().split(':')\n",
        "    lemma = v[0].strip()\n",
        "    tokens = v[1].strip().split(\" \")\n",
        "    # print(lemma, \":\", tokens)\n",
        "    # print(tokens)\n",
        "    token_count = 0\n",
        "    idf = float(0)\n",
        "    for word in tokens:\n",
        "      token_count += token_tf[word]\n",
        "      idf += math.log10(files_count/token_inverted_index_count[word])\n",
        "\n",
        "    tf = token_count / float(token_list_len)\n",
        "\n",
        "    lemma_tf[lemma] = [idf, tf * idf]\n",
        "\n",
        "  with open('/content/drive/My Drive/' + write_folder + '/lemmas-' + key + '.txt', 'w') as f:\n",
        "    for lemma, metrics in lemma_tf.items():\n",
        "      result_string = lemma + \" \" + str(metrics[0]) + \" \" + str(metrics[1])\n",
        "      f.write(\"%s\\n\" % result_string)\n",
        "  print(key)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
